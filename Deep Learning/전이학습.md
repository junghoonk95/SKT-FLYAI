# 전이 학습

- 기존에 사전학습된 (pre trained)모델을 가져와, 사용하고자 하는 학습 데이터를 학습시켜 이용하는 방법

- 기존에 비슷한 도메인의 데이터를 학습한 모델이라면 현재 갖고있는 데이터가 다소 적더라도 좋은 성능을 보여줌

<img src="https://user-images.githubusercontent.com/80855939/210673760-009de1dc-40e1-4d5e-8978-bebd78579d8e.png" width=70% height=70%>

- 전이학습을 사용하는 이유
  - 학습이 빠르게 수행
    - 이미 입력되는 데이터에 대한 특징을 효율적으로 추출하기 때문에, 학습할 데이터에 대해 특징을 추출하기 위한 학습을 별도로 하지 않아도 됨
   
  - 작은 데이터셋에 대해 학습할 때 오버피팅을 예방
    - 적은 데이터로 특징을 추출하기 위한 학습을 하게 되면, 데이터 수에 비해 모델의 가중치 수가 많을 수 있어 미세한 특징까지 모두 학습.
    - 전이 학습을 이용해 마지막 레이어만 학습하게 한다면, 학습할 가중치 수가 줄어 과한 학습이 이루어지지 않게 할 수 있음
    - 
- Fine Tuning
  - 모델을 불러와 동결해 두었던 전이학습 모델의 가중치를 (일부 또는 전부) 학습 가능상태로 만들고 학습. 
  - (일부 또는 전체) 부분은 정해진 답이 없어 딱 잘라 말할 수 없음  

- 전이학습의 사전 학습과 미세조정
  - 사전 학습(pre - training)
    - 학습된 모델을 만드는 과정
    
  - 미세조정(fine - tuning) 
    - 사전 학습된 모델을 새로운 문제에 적용하기 위해 일부 가중치를 조절하는 학습 과정
    - 모델을 불러와 동결해 두었던 전이학습 모델의 가중치를(일부 또는 전부) 학습 가능상태로 만들고 학습
    - (일부 또는 전체) 부분은 정해진 답이 없어 딱 잘라 말할 수 없음

- 전이학습 성능 개선 -> Fine Tuning
- 추가 성능 개선
  - 옵티마이저의 변경
  - BatchNormalization 추가
  - Dropout 추가
  - resnet50 모델까지 전체 학습 -> 구조만 가져오고 전부 학습하는 것을 의미
  - Data 전처리 및 증강을 통해 학습데이터 개선
  - Fully Connected Layer 은닉층 추가 및 노드 추가
  - learning decay를 이용해 유동적인 학습 진행 
